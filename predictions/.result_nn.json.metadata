{"timestamp": 1687186203.326307, "stored_source_code": "upstream = None\nproduct = None\nimport json\nimport pickle\nimport warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.optim.lr_scheduler import StepLR, ExponentialLR, ReduceLROnPlateau\nfrom torchmetrics import F1Score\n\nwarnings.filterwarnings('ignore')\ncols = ['is_beverage', 'non_recyclable_and_non_biodegradable_materials_count', \n        'est_co2_agriculture', 'est_co2_consumption', \n        'est_co2_distribution', 'est_co2_packaging', 'est_co2_processing', \n        'est_co2_transportation']\ntarget_col = 'ecoscore_grade'\ntrain = pd.read_csv(upstream['Preprocess features']['train_csv'], usecols=cols + [target_col])\ntest = pd.read_csv(upstream['Preprocess features']['test_csv'], usecols=cols)\nX_train, y_train = train[cols], train[target_col]\nX_test = test[cols]\nfor i in cols:\n    X_train[i] = X_train[i].astype(float)\n    X_test[i] = X_test[i].astype(float)\nbatch_size = 32 * 2\ntrain_loader = DataLoader(\n    TensorDataset(torch.tensor(X_train.values, dtype=torch.float32), \n                  torch.tensor(y_train.values, dtype=torch.float32)), \n    batch_size=batch_size, shuffle=True,\n)\n\nclass Net(nn.Module):\n    activation_function: nn.modules.activation = None\n\n    def __init__(self, input_shape, output_shape, activation_function, hidden_layers: list = [160], dropout=0.6):\n        super(Net, self).__init__()\n\n        activation_function = getattr(nn, activation_function)\n        layers = []\n        hidden_layers.insert(0, input_shape)\n        for i, n in enumerate(hidden_layers[0:-1]):\n            m = int(hidden_layers[i + 1])\n            layers.append(nn.Linear(n, m))\n            layers.append(nn.BatchNorm1d(m))\n            layers.append(nn.Dropout(dropout))\n            layers.append(activation_function())\n        layers.append(nn.Linear(hidden_layers[-1], output_shape))\n        layers.append(nn.Sigmoid())\n        layers = nn.Sequential(*layers)\n        self.layers = layers\n    def forward(self, x):\n        x = self.layers(x)\n        return x\n\n    def predict(self, x):\n        x = torch.from_numpy(x).float().to(\"cpu\")\n        outputs = self(x)\n        return outputs\nmetric_f1 = F1Score(task='multiclass',num_classes=len(y_train.unique()), average=\"weighted\", multiclass=True)\nnetwork = Net(X_train.shape[1], len(y_train.unique()), 'ReLU6', [128, 64, 32, 16])\noptimizer = optim.Adam(network.parameters(), lr=0.05)\ncriterion = nn.CrossEntropyLoss()\nscheduler = ReduceLROnPlateau(optimizer, \"max\", factor=0.9)\nepochs = 100\nfor epoch in range(epochs):\n    running_loss = 0\n    f1 = 0\n    \n    for (data, target) in train_loader:\n        optimizer.zero_grad()\n        output = network(data)\n\n        target = nn.functional.one_hot(target.to(torch.int64), num_classes=len(y_train.unique()))\n        loss = criterion(output.float(), target.float())\n\n        ## Do backward\n        loss.backward()\n        optimizer.step()\n\n        f1 += metric_f1(torch.argmax(output, axis=1), torch.argmax(target.int(), axis=1)).item()\n        running_loss += loss.item()\n    f1 /= len(train_loader)\n    running_loss /= len(train_loader)\n    # scheduler.step(f1)\n    print(f'Epoch: {epoch} | F1: {f1} | Loss: {running_loss}')\nnetwork.eval()\npred = np.argmax(network.predict(X_test.values).detach().numpy(), axis=1)\n\n\nPath(product['result'])\\\n    .write_text(json.dumps(\n        {'target': {index: int(i) \n                    for index, i in enumerate(pred)}\n         }))", "params": {}}